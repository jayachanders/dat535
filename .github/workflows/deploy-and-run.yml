name: Data Pipeline on Self-Hosted Runner

on:
  push:
    branches: [ main ]

jobs:
  pipeline:
    runs-on: self-hosted

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Python Environment
        run: |
          # Check if venv exists and is valid, if not recreate it
          if [ ! -f /home/ubuntu/spark-env/bin/pip ]; then
            echo "Virtual environment is missing or corrupted. Recreating..."
            rm -rf /home/ubuntu/spark-env
            python3 -m venv /home/ubuntu/spark-env
            echo "Virtual environment created"
          fi
          
          # Install/upgrade dependencies
          /home/ubuntu/spark-env/bin/python -m pip install --upgrade pip
          /home/ubuntu/spark-env/bin/pip install findspark pyspark pandas numpy matplotlib seaborn
          echo "Dependencies installed/verified in virtual environment"

      - name: Run Spark Pipeline
        env:
          SPARK_HOME: /opt/spark
          JAVA_HOME: /usr/lib/jvm/java-8-openjdk-amd64
        run: |
          # Activate the environment
          source ~/spark-env/bin/activate
          
          # Verify setup
          echo "Python: $(which python)"
          echo "Python version: $(python --version)"
          echo "SPARK_HOME: $SPARK_HOME"
          
          # Run the pipeline
          python $GITHUB_WORKSPACE/process_data.py